{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Exercise 1: Linear regression with multiple variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and explore data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('ex1data2.txt', header=None, names=['x1', 'x2', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     x1  x2       y\n",
       "0  2104   3  399900\n",
       "1  1600   3  329900\n",
       "2  2400   3  369000\n",
       "3  1416   2  232000\n",
       "4  3000   4  539900"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47, 3)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[['x1', 'x2']].values\n",
    "Y = data['y'].values\n",
    "m = len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Feature Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_normalize(X):\n",
    "    #   FEATURENORMALIZE Normalizes the features in X \n",
    "    #   FEATURENORMALIZE(X) returns a normalized version of X where\n",
    "    #   the mean value of each feature is 0 and the standard deviation\n",
    "    #   is 1. This is often a good preprocessing step to do when\n",
    "    #   working with learning algorithms.\n",
    "\n",
    "    # You need to set these values correctly\n",
    "    X_norm = X\n",
    "    mu     = np.zeros(X.shape[1])\n",
    "    sigma  = np.zeros(X.shape[1])\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: First, for each feature dimension, compute the mean\n",
    "    #               of the feature and subtract it from the dataset,\n",
    "    #               storing the mean value in mu. Next, compute the \n",
    "    #               standard deviation of each feature and divide\n",
    "    #               each feature by it's standard deviation, storing\n",
    "    #               the standard deviation in sigma. \n",
    "    #\n",
    "    #               Note that X is a matrix where each column is a \n",
    "    #               feature and each row is an example. You need \n",
    "    #               to perform the normalization separately for \n",
    "    #               each feature. \n",
    "    #\n",
    "    # Hint: You might find the 'np.mean' and 'np.std' functions useful.\n",
    "    #  \n",
    "    mu = np.mean(X)\n",
    "    sigma = np.std(X)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    \n",
    "    return X_norm, mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale features and set them to zero mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_norm, mu, sigma = feature_normalize(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add intercept term to X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.96415008, -0.87391021],\n",
       "       [ 1.        ,  0.52322557, -0.87391021]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm = np.insert(X_norm, 0, 1, 1)\n",
    "X_norm[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose some alpha value\n",
    "alpha = 0.01\n",
    "# Init Theta\n",
    "theta = np.zeros(3)\n",
    "\n",
    "iterations = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure your implementations of compute_cost and gradient_descent work when X has more than 2 columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost_multi(X, y, theta):\n",
    "    # COMPUTECOSTMULTI Compute cost for linear regression\n",
    "    # J = COMPUTECOSTMULTI(X, y, theta) computes the cost of using theta as the\n",
    "    # parameter for linear regression to fit the data points in X and y\n",
    "    \n",
    "    # some useful values\n",
    "    m = len(X)\n",
    "    \n",
    "    # You need to return this value correctly:\n",
    "    J = 0\n",
    "    \n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Compute the cost of a particular choice of theta\n",
    "    #               You should set J to the cost.\n",
    "    y_hat = theta.dot(X.T)\n",
    "    J = (1/(2*m)) * (y_hat - y).T.dot(y_hat - y)\n",
    "    \n",
    "    \n",
    "    # ============================================================\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost at initial theta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65591548106.457443"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost_multi(X_norm, Y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent_multi(X, y, theta, alpha, num_iters):\n",
    "    # GRADIENTDESCENT Performs gradient descent to learn theta\n",
    "    # theta = GRADIENTDESCENT(X, y, theta, alpha, num_iters) updates theta by \n",
    "    # taking num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    # Initialize\n",
    "    J_history = np.zeros(num_iters)\n",
    "    T_history = np.zeros((num_iters,X.shape[1]))\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        T_history[i] = theta\n",
    "\n",
    "        ### ========= YOUR CODE HERE ============\n",
    "        # Instructions: Perform a single gradient step on the parameter vector theta.\n",
    "        # θ := θ −mXT(Xθ − y)\n",
    "        y_hat = theta.dot(X.T)        \n",
    "        y_hat_minus_y = (y_hat - y)\n",
    "        delta = X.T.dot(y_hat_minus_y)\n",
    "        theta = theta - (alpha/m) * X.T.dot(y_hat_minus_y).T\n",
    "        ### =====================================\n",
    "        \n",
    "        J_history[i] = compute_cost_multi(X, y, theta)\n",
    "    return theta, J_history, T_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (3,) and (47,3) not aligned: 3 (dim 0) != 47 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-dc31637c0616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_descent_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-152-b204c88da4e0>\u001b[0m in \u001b[0;36mgradient_descent_multi\u001b[0;34m(X, y, theta, alpha, num_iters)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Instructions: Perform a single gradient step on the parameter vector theta.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# θ := θ −mXT(Xθ − y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0my_hat_minus_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_minus_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (3,) and (47,3) not aligned: 3 (dim 0) != 47 (dim 0)"
     ]
    }
   ],
   "source": [
    "theta, J_history, T_history = gradient_descent_multi(X_norm, Y, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theta values found by gradient descent should be [ 340412.65957447,  109447.79646964,   -6578.35485416])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 119999.41363909,  148539.68555667, -104839.19814394])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convergence graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1138549e8>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEDCAYAAAAVyO4LAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136ee828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandas.Series(J_history).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the price of a 1650 sqft, 3 bedroom house:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Estimate the price of a 1650 sq-ft, 3 br house\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "# Recall that the first column of X is all-ones. Thus, it does\n",
    "# not need to be normalized.\n",
    "\n",
    "price = 0\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Normal Equations \n",
    "The following code computes the closed form \n",
    "solution for linear regression using the normal\n",
    "equations. You should complete the code in \n",
    "normal_eqn().\n",
    "\n",
    "After doing so, you should complete this code \n",
    "to predict the price of a 1650 sq-ft, 3 br house.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('ex1data2.txt', header=None, names=['x1', 'x2', 'y'])\n",
    "X = data[['x1', 'x2']].values\n",
    "Y = data['y'].values\n",
    "X = np.insert(X, 0, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normal_eqn(X, y):\n",
    "    #NORMALEQN Computes the closed-form solution to linear regression \n",
    "    #   NORMALEQN(X,y) computes the closed-form solution to linear \n",
    "    #   regression using the normal equations.\n",
    "\n",
    "    theta = np.zeros(X.shape[1]);\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    # Instructions: Complete the code to compute the closed form solution\n",
    "    #               to linear regression and put the result in theta.\n",
    "    #\n",
    "\n",
    "\n",
    "\n",
    "    # ============================================================\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = normal_eqn(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theta found using the normal equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price estimation of a 1650sqft house with 3 bedrooms, using theta from the normal equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ====================== YOUR CODE HERE ======================\n",
    "0\n",
    "# ============================================================"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
